# 1. Data Cleaning

### 데이터 정제

- 부정확한 레코드 감지 및 수정하는 프로세스
- **유효성, 정확도, 완전성, 일관성**을 유지하고 신뢰성을 향상시키는 중요한 과정

### 데이터 전처리와 차이점

- **데이터 전처리** : 데이터를 주어진 양식에 맞게 변경/수정하는 과정
- **데이터 정제** : 데이터의 부정확한 레코드 감지 및 수정, 데이터 전처리 단계 이전에 수행

---

<br>

# 2. 데이터 통합

데이터 분석은 다양한 기관/위치에서 수집한 데이터를 하나로 모아 진행하기 때문에 다양한 소스에 존재하는 데이터를 하나로 합치는 작업이 필요하다.

![데이터통합](https://github.com/user-attachments/assets/a8f48e13-96d2-4074-b971-177ccc2c6583)

<br>

### 단순 데이터 붙이기

여러 개의 데이터 파일이 있는 경우, row 혹은 column을 결합하는 작업

**Concatenate**

    단순히 배열을 합치는 데 사용
    - 같은 크기의 column이나 row를 갖는 배열을 하나의 데이터로 합친다.
    - feature는 고려하지 않고 데이터의 크기만을 비교하여 합친다.

<br>

### 데이터 병합 (Data Merge)

두 개 이상의 데이터 세트를 하나의 데이터 세트로 병합하는 프로세스
> 데이터 병합은 데이터 관리를 쉽게 만들고 더 많은 정보와 의미를 제공함으로써 데이터 분석을 더욱 효과적으로 수행할 수 있도록 해준다.

**데이터 병합 방법**

    일반적으로 특정 키 값을 기준으로 수행.
        - 데이터 사이에 공통된 키 값을 통해 두 개 이상의 데이터 세트를 매칭할 수 있다.

**데이터 조인**

|조인 방법|설명|
|---|---------|
|inner join|가장 보편적으로 사용하는 방법으로, key를 기준으로 두 테이블에 같이 존재하는 데이터 추출|
|left outer join|왼쪽 데이터셋에 있는 테이블의 key를 기준으로 병합|
|right outer join|오른쪽 데이터셋에 있는 테이블의 key를 기준으로 병합|
|full outer join|key를 기준으로 두 테이블에 존재하는 모든 데이터를 병합|

![Inner Join](https://github.com/user-attachments/assets/b0337e68-d6c9-4f4b-bb74-7498c7d0d551)
![Left(Right) Outer Join](https://github.com/user-attachments/assets/3c983d20-6636-42c6-87a7-7f3b3c0f3261)
![Full Outer Join](https://github.com/user-attachments/assets/169f7537-1af1-4ee5-bc36-74d2d9238b96)

<p align='center'>
    <img src="https://github.com/user-attachments/assets/b0337e68-d6c9-4f4b-bb74-7498c7d0d551" width="200"/>
    <img src="https://github.com/user-attachments/assets/3c983d20-6636-42c6-87a7-7f3b3c0f3261" width="200"/>
    <img src="https://github.com/user-attachments/assets/169f7537-1af1-4ee5-bc36-74d2d9238b96" width="200"/>
</p>

### [데이터 병합 실습]()

<br>

### 데이터 통합 프로세스

데이터 통합은 데이터 분석 및 비즈니스 의사 결정에 중요한 역할을 한다.

#### 1. 통합 데이터 선정
- 어떤 데이터를 통합할 지 선정하는 과정
- 비즈니스 / 과제의 요구 사항에 따라 통합 데이터를 선정

#### 2. 데이터 표준사전 정의
- 수집 데이터에 대한 표준 어휘를 사용하여 데이터의 속성을 정의
    - 표준을 사용하지 않으면 사람마다 나름대로의 규칙과 습관에 따라 테이블이 생성됨
- 데이터 표준은 용어, 단어, 도메인 세 가지를 정의함

#### 3. 중복데이터 판단
- 중복 데이터는 데이터의 정확도와 신뢰성을 저해할 수 있으며, 데이터 분석 결과에 영향을 미칠 수 있으므로 데이터 정리 및 전처리 과정에서 반드시 확인해야 한다.
- 중복 데이터는 **인스턴스 중복**과 **속성의 중복** 2가지가 발생할 수 있다.

**인스턴스 중복**
- 데이터 세트에서 동일한 레코드가 두 번 이상 나타나는 경우
    ```python
    # 중복 데이터 확인
    df.duplicated()

    # 중복 데이터 제거
    df.drop_duplicates(inplace = True)
    ```

**속성의 중복**
- 데이터 세트에서 동일한 속성이 두 번 이상 나타나는 경우
- 속성의 중복 감지

        - 수치형 데이터 : 특성 간 상관관계 분석 실시
        - 범주형 데이터 : 카이제곱검정을 계산하여 평가

### [데이터 중복 제거 실습]()

---

<br>

# 3. 결측치 처리

### 결측치
관측을 통해 데이터 수집을 실행했지만, 데이터 값이 없는 경우
>NaN 혹은 Null값 형태로 존재

### 결측치 발생 원인
1. 데이터 수집/관리 과정에서 누락 된 경우
2. 시간이 지나면서 새롭게 수집한 항목이 있는 경우
3. 서로 다른 속성의 데이터를 통합한 경우

### 결측치를 처리하는 이유
- 결측치 처리를 통해 데이터의 신뢰성을 높일 수 있음
- 잘못 처리할 경우 데이터 손실이나 편향이 생길 수 있음

<br>

## 결측치 삭제

결측치를 제거하는 가장 간단한 방법
- 하지만 데이터의 양이 적을 경우, 유용하지 않을 수 있다.

**결측치 삭제 기준**
- 10% 미만 : data 삭제 또는 치환
- 10~50% : 모델을 만들어 처리
- 50% 이상 : column 삭제

**결측치 삭제 방식**
- 목록 삭제
- 부분 삭제
- 속성변수 삭제

### 목록 삭제(Listwise Deletion)
결측치가 발생한 행 자체를 삭제

![목록삭제](https://github.com/user-attachments/assets/a9329266-28bc-47ca-a75e-18363f70b257)

### 부분 삭제(Pairwise Deletion)
결측치를 삭제하고 빈칸으로 사용하는 경우 다른 feature에 결측치가 있어도 분석에 필요한 변수가 채워져 있으면 그대로 분석에 사용하는 방법

![부분삭제](https://github.com/user-attachments/assets/e3d7d01c-c2a3-4eb5-af7e-74933d0921a0)

### 속성변수 삭제(Dropping Variables)
row를 삭제하는 것이 아니라 column 삭제

![속성변수삭제](https://github.com/user-attachments/assets/9e94ffe0-2476-4701-9fe0-e5c3ad91de10)

<br>

## 결측치 대체 (Imputation)

결측치에 대해 합리적인 값으로 결측치를 대체하는 방법

**결측치 대체 방식**
1. 평균값, 중앙값, 최빈값 등의 대표값으로 대체
2. 선형 보간법 등의 수치 예측 방법을 사용하여 대체
3. 결측치 모델을 사용하여 대체
    - KNN, 결정 트리, 랜덤 포레스트, 신경망 등

### [결측치 제거 및 대체 실습]()

---

<br>

# 4. 정규화

### 두 개 이상의 변수 탐색
두 개 이상의 변수를 탐색할 경우, 각각 변수를 앞서 소개한 통계분석 방법을 이용하여 분석 후 비교해도 된다.

하지만 정확한 비교를 위해서, 그리고 모델의 올바른 훈련을 위해서 Scaling을 해줘야 한다.

데이터의 크기가 들쑥날쑥 하다면 데이터를 이상하게 해석할 우려가 있다.

### 정규화
모든 기준의 척도를 통일화 하는 과정
>즉 모든 데이터가 같은 정도의 중요도로 반영되도록 하는 것

**데이터들 간의 호환 문제**
- ex) 60이란 데이터가 있을 때, height에서는 작은 값으로, weight에서는 큰 값으로 인식되어야 한다.

이를 해결하기 위해 *1) 데이터의 간격을 늘리거나 줄이는 방법(정규화, Normalization)* 이 있고, *2) 데이터 간격은 유지시키고 데이터의 척도를 공통 척도로 바꾸는 방법(표준화, Standardizaion)* 이 있다.

<br>

## 정규화 (Normalization)

모든 변수들의 크기를 0 ~ 1.0 사이로 Scaling 하는 방법

$X_{norm} = \frac{X-X_{min}}{X_{max}-X_{min}}$

>최소값은 0, 최대값은 1로 만들고 이 사이에 데이터들이 배치 된다.

>데이터의 최소, 최대값이 사용되므로 Min-Max Normalizaion이라고도 한다.

![정규화](https://github.com/user-attachments/assets/d64a04db-806e-40d3-9a22-5f3da7cac68f)

<br>

## 표준화 (Standardizaion)

데이터의 평균을 0, 표준편차를 1로 조정하는 방법

$Z_{score} = \frac{X-\mu}{\sigma}$

>정규화는 Scale을 통일하지만, 표준화는 데이터 범위를 통일한다.

![표준화](https://github.com/user-attachments/assets/70edf1a2-2331-4126-9b32-575b08ab11b7)

<br>

### 정규화와 표준화 비교

||장점|단점|
|--------|-----------|---------------|
|정규화|모든 데이터를 0 ~ 1 사이로 제한하여 모델 학습에 있어 각 특성이 동등하게 고려될 수 있도록 함|이상치에 매우 민감하여 이상치가 있는 경우 데이터 범위가 크게 왜곡됨|
|표준화|이상치에 영향을 덜 받으며, 데이터 분포가 평균 주변으로 어떻게 퍼져있는지를 반영함|모든 데이터 포인트가 동일한 범위에 있지 않기 때문에 중요도가 다를 수 있음|

**표준화 후 정규화를 이용하자!**

---

<br>

# 5. 노이즈 제거

### 노이즈
실제 입력된 신호가 아닌 것

### 노이즈의 종류
- 화이트 노이즈 : 일정 규칙 없이 랜덤하게 발생하는 잡음
- 통계적 노이즈 : 통계적으로 분포를 따르는 잡음

### 노이즈 처리 방법
- Moving Average Filter : 이동하면서 주위 값들에 비해 높거나 낮을 경우 평균값으로 대체
- Median Filter : 일정 범위의 중간 값을 해당 지점의 값으로 지정
- Bandpass Filter : 주파수 기반의 필터