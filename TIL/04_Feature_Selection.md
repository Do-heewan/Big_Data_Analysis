# Feature Selection

### [Feature Selection 실습](https://github.com/Do-heewan/Big_Data_Analysis/blob/main/04_Feature_Selection/Feature_Selection.ipynb)

<br>

### 특성 선택 (Feature Selection)이란?
- 데이터의 특성 중에서 **가장 중요하게 생각되는 변수를 선택**하는 것
- 학습 모델 훈련에 가장 유용한 특성을 선택하는 것이 목표이다.

### 이점
- 차원의 저주를 줄임
- 모델의 성능을 높임
- 과적합 방지
- 계산 효율 증가
- 데이터 분석에서 중요하지 않은 특성 제거

> **Feature Selection에는 지도 학습과 비지도 학습이 있다. 비지도 학습에서는 학습에 필요없는 데이터를 제거하는 것이 대부분이고, 지도 학습에는 Filters, Wrappers, Embedded 세 가지 기법이 있다.**

<br>

## Filter Method

### Filter Method란?
- 데이터셋에서 각 Feature들의 통계적인 특성을 이용하여 Feature들의 중요도를 평가하고, 이를 기반으로 Feature를 선택하는 방법

### 원리
- 기준값(Label)과 변수들 간에 관계성을 분석하여 관계가 깊은 변수를 중요한 변수로 간주하여 선택

<br>

### 1. 카이제곱검정 (Chi Squared Test)
- **범주형 데이터**에서 사용되는 방법
- 임의의 feature를 선택하고 다른 feature간 독립성을 검정하여 feature의 중요도를 측정하는 방법

기댓값과 관측값을 기댓값으로 나눈 값들의 합 = 카이제곱 통계량
이 수치가 자유도에 대한 확률 분포의 수치보다 작다면? -> 유의미한 관계가 있다고 보기 어렵다.
따라서 제거해도 될까라는 가설은 통과. 이 데이터는 버려도 된다.

<br>

### 2. 상관 계수 (Correlation)

#### 상관계수 기반의 변수 선택
- 데이터간 상관계수를 구한 후 상관계수가 높을 경우 해당 변수를 선택

#### 상관계수란?
- 두 변수 사이에 관계의 정도를 나타내는 수치
- [-1, 1] 사이의 값을 가지며, -1에 가까울 수록 음의 상관관계에 가깝고, +1에 가까울 수록 같은 데이터라고 볼 수 있다. 0에 가까울 수록 둘 사이의 관계가 없다.

![상관계수](https://github.com/user-attachments/assets/5333c089-3d75-400e-8ba0-9c26e1aa8f78)

#### 장점
- 간단하고 직관적
- 변수 간 관계 파악이 쉽다
- 계산이 단순

#### 단점
- 선형적 관계를 기반으로 하기 때문에 비선형적인 관계 분석이 어려움
- 연관성을 보여주지만 인과관계를 설명하지는 않음
- 누락된 데이터나 이상치에 대해 민감한 반응

<br>

### 3. 상호정보량 (Mutual Information)

#### 상호정보량이란?
- 두 확률 변수 간 의존성을 나타내는 척도
- 두 변수간 의존성이 클 경우 해당 변수를 선택

![상호정보량](https://github.com/user-attachments/assets/810a2616-a450-4f6c-ba5a-5402f35a6c64)

- 교집합 부분이 상호정보량.
- 두 집합의 합이 상관관계에 대한 엔트로피
- 엔트로피가 작다(상관관계가 강하다) -> 겹치는 부분이 크다 -> 상호정보량이 크다

#### 상호정보량을 이용한 변수 선택 과정
    1) 각 특성과 목표 변수 간의 상호정보량 계산
    2) 계산된 상호정보량을 기준으로 정렬
    3) 임계값을 기준으로 상위 순위의 특성을 선택

#### 장단점
- 다른 변수 선택 방법보다 정확함
- 비선형적 관계의 변수도 분석 가능
- 비대칭성을 고려할 수 있으므로, 두 변수 간의 관계가 양방향인지, 단방향인지 구분 가능
- 데이터가 희소한 경우에도 가능
- 계산 비용이 크다는 단점 존재

<br>

## Wrapper Method

### Wrapper Method란?
- 모델을 학습시키면서 변수를 선택하는 방법
- 특정 변수 조합에 대해 모델을 학습시켜 성능을 평가한 후 다음 라운드에서 특정 변수를 추가하거나 제거한 뒤 학습해가면서 최종 변수를 선택하는 방법

#### 대표적인 방법
- 후진 소거법 (Recursive Feature Elimination, RFE)

#### 장점
- 모델 성능을 향상시키는 최상의 변수 조합을 선택할 수 있음
- 과적합 방지를 위해 교차 검증과 같은 일반화 기술을 적용할 수 있음

#### 단점
- 계산비용이 높으며 시간이 오래 걸림

<br>

### 후진 소거법 (Recursive Feature Elimination, RFE)
- 가장 널리 사용하는 방법 중 하나
- 모든 변수를 다 포함시킨 후 학습을 진행하고 중요도가 낮은 변수를 하나씩 제거하는 방식

#### 실행 과정
    1) 모든 특성을 포함하는 모델 선택
    2) 선택한 모델을 사용하여 특성의 중요도를 계산
    3) 중요도가 가장 낮은 특성 제거
    4) 특성 제거 후 남은 특성으로 새로운 모델 학습
    5) 1~4 과정을 반복하여 미리 지정한 특성의 개수나 중요도에 따라 특성 선택

#### 장단점
- 모델의 성능을 극대화 하는  특성 집합을 찾아낼 수 있음
- 특성의 중요도를 판단할 수 있음
- 특성을 선택하는데 시간이 오래 걸린다는 것과 모델에 따라 결과가 달라질 수 있다는 단점
