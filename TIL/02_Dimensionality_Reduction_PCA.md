# Dimensionality Reduction 01: PCA

### [PCA 실습](https://github.com/Do-heewan/Big_Data_Analysis/blob/main/02_PCA/PCA.ipynb)

<br>

### 차원의 저주
차원이 증가하면서 문제 공간이 지수적으로 커지는 현상
>차원이 커질수록 공간은 많이 필요하면서, 사용할 수 있는 정보량은 상대적으로 작아지는 현상

### 차원 축소란?
고차원의 데이터를 저차원의 데이터로 변환하는 방법
>차원 : 데이터 분석에 사용되는 변수의 개수 -> 즉 사용되는 변수를 줄이는 방법

### 차원 축소를 하는 이유
- 비용, 시간, 자원, 용량 문제 해결
- 과적합 문제 해결
- 설명력 증가

### 차원 축소의 단점
- 정보의 손실

### 대표적인 차원 축소 알고리즘
- **주성분 분석(PCA)**
- LDA
- LLE
- MDS
- Isomap
- **t-SNE**

<br>

## PCA를 배우기전에 알아야 할 개념
	1. 정사영 / 투영(projection)
	2. 차원축소와 분산
	3. 공분산

### 1. 정사영/투영
3차원 -> 2차원 투영 (약간의 정보 손실) <br>
3차원 -> 1차원 (기저(basis) or 초평면(hyperplane))

### 2. 차원축소와 분산
- 분산의 정의 : 데이터가 (평균으로 부터) 퍼져있는 정도
- 정보이론적 관점 : 정보량을 의미

**엔트로피(Entropy)**
- 정의 : 물질이 분산되는 정도 / 분포가 가지는 정보의 확신도 (=정보량)

**차원축소와 분산**
- 두 개 이상의 feature를 합쳐 하나로 만들 때 **데이터의 분산을 유지하면 데이터의 특성을 유지할 수 있다.**
- 고차원의 분산과 저차원의 분산이 같다면, 두 데이터는 같은 데이터다.
	- 데이터 간의 분산을 최대로 한다면 정보의 손실을 최소화 할 수 있다.

### 3. 공분산
- 정의 : 2개의 확률변수의 **선형관계/상관정도(correlation)** 을 나타내는 값 <br>
증, 증 -> True (증가) <br>
증, 감 -> False (감소) <br>
감, 증 -> False <br>
감, 감 -> True <br>

>공분산을 통해 상관관계의 방향을 알 수 있다.

**공분산 법칙**

![공분산법칙](https://github.com/user-attachments/assets/22491682-668c-4347-9598-ede82f2464b5)

<br>
<br>

# 주성분 분석 (PCA, Principal Component Analysis)

### 개요
	- 분산이 가장 큰 방향으로 첫 번째 축을 생성하여 사영
	- 첫 번째 축에 직각이 되는 벡터를 두 번째 축으로 하여 사영 (두번째 feature)
	- 두 번째 축에 직각이 되는 벡터를 세 번째 축으로 하여 사영 (세번째 feature)

### 목적
- 고차원의 데이터를 분산이 최대로 보존되는 저차원의 축 평면으로 투영하여 차원 축소

### 해결하고자 한 것
- 분산이 최대로 보존되는 축을 어떻게 찾을 것인가

### 방법
- 입력 데이터들의 공분산 행렬에 대한 고유벡터, 고유값을 구하고 그 값으로 선형 변환(투영)을 취한다.

<br>

## 고유값과 고유벡터

#### 정의 
어떤 행렬 A에 대하여 상수 `lambda`와 벡터 `x`가 다음 식을 만족한다면,

### $Ax = \lambda x$

이때 `lambda`와 `x`를 각각 행렬 A의 고유값 및 고유벡터라고 한다.

### 선형 변환의 결과가 다음과 같다면

![선형변화](https://github.com/user-attachments/assets/844ea947-4c7f-4461-9464-e16774f9d42d)

- 벡터의 방향은 유지, 크기만 변경
- 고유벡터는 선형 변환을 할 때 어떤 방향으로 얼마나 힘이 가해지는지를 표현
- *어떤 방향으로 분산이 커지는지를 구하는 것*

#### 고유 벡터(x)
- 행렬이 벡터의 변화에 작용하는 주축의 방향을 나타냄
- 공분산 행렬의 고유벡터는 데이터가 어떤 방향으로 분산되어 있는지 나타냄

#### 고유 값
- 고유벡터 방향으로 얼마 만큼의 크기로 벡터공간이 늘려지는지를 나타냄
- 고유값은 여러 해가 존재할 수 있으며, 값이 큰 순서대로 고유 벡터를 정렬하면 결과적으로 중요한 순서대로 주성분을 구할 수 있음

<br>

### 주성분 분석 과정 요약

	1) 데이터 정규화(표준화)
	2) Covariance Matrix 계산
	3) Covariance Matrix로 부터 고유값 계산
	4) 고유 벡터 구하기
	5) 고유벡터를 고유값의 순서대로 나열
	6) 고유벡터를 1)에서 구한 기존 데이터에 적용하여 변환 

<br>

### 주성분 분석 한계

데이터의 분포가 비선형일 경우 적용하기 어렵다.

분류 문제에 대해서 데이터의 범주 정보를 고려하지 않기 때문에 PCA를 취한 뒤 범주 분류가 잘 되도록 변환되는 것은 아님